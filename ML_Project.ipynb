{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# AMFD_Dataframe = pd.read_excel('AMFD_Norming_Data_Table.xlsx', skiprows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################Stracth Part############################################\n",
    "# import torch\n",
    "# from torchvision.transforms import Compose, Resize, ToTensor\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# # image paths\n",
    "# image_dir = \"AMFD_Faces_Final\"\n",
    "# image_paths = []\n",
    "# for filename in os.listdir(image_dir):\n",
    "#     if filename.endswith('.jpg'):\n",
    "#         image_paths.append(os.path.join(image_dir, filename))\n",
    "\n",
    "# # photoid = 1001+idx\n",
    "# idx = 4\n",
    "\n",
    "# # print idx-th image\n",
    "# image_path = image_paths[idx]\n",
    "# print(image_path)\n",
    "\n",
    "# # Features tensor get\n",
    "# features = torch.tensor(AMFD_Dataframe.iloc[idx, 0:].values, dtype=torch.float32)\n",
    "# print(features)\n",
    "# print(len(features))\n",
    "\n",
    "# # Image ID get\n",
    "# image_id = AMFD_Dataframe.iloc[idx, 0]\n",
    "# print(image_id)\n",
    "\n",
    "# # Expression and Gender get as label\n",
    "# #image_path = \"AMFD_Faces_Final\\SF-1040.jpg\"\n",
    "# parts = image_path.split('\\\\')[-1].split('-')\n",
    "# expression_code = parts[0][0]\n",
    "# gender_code = parts[0][1]\n",
    "# expression_label = 0 if expression_code == 'N' else 1\n",
    "# gender_label = 0 if gender_code == 'F' else 1\n",
    "# print(f\"{image_path}'s expression code is {expression_code} and gender code is {gender_code}\")\n",
    "\n",
    "# # image to tensor\n",
    "# image = Image.open(image_path).convert('RGB')\n",
    "# image_tensor = ToTensor()(image)\n",
    "# print(image_tensor)\n",
    "\n",
    "# # check if image is all white or not\n",
    "# any_value_over_1 = (image_tensor == 1).all().item()\n",
    "# max_value = image_tensor.max().item()\n",
    "# any_value_over_1, max_value\n",
    "\n",
    "#########################################Stracth Part############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read excel table and set data to data frame\n",
    "AMFD_Dataframe = pd.read_excel('AMFD_Norming_Data_Table.xlsx', skiprows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# put all image path to a list\n",
    "image_dir = \"AMFD_Faces_Final\"\n",
    "image_paths = []\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith('.jpg'):\n",
    "        image_paths.append(os.path.join(image_dir, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Study\\Softwares\\Anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Study\\Softwares\\Anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AMFDDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_paths, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_path = self.image_paths[idx]\n",
    "\n",
    "        # extract element from the image name\n",
    "        parts = image_path.split('\\\\')[-1].split('-')\n",
    "\n",
    "        # get expression code (which is the Ftype in table)\n",
    "        expression_code = 0 if parts[0][0] == 'N' else 1\n",
    "\n",
    "        # get image id from image file\n",
    "        image_id_photo = parts[1].split('.')[0]\n",
    "\n",
    "        # using expression code and image id to seach in dataframe\n",
    "        match = self.dataframe[(self.dataframe['PhotoID'] == int(image_id_photo)) & (self.dataframe['FType'] == expression_code)]\n",
    "        \n",
    "        # get the feature from the matching line\n",
    "        image_features = None\n",
    "        if not match.empty:\n",
    "            # this will return the whole line \n",
    "            # if we need photo_id included, use match.iloc[0, 0:].values, otherwise match.iloc[0, 1:].values\n",
    "            image_features = torch.tensor(match.iloc[0, 0:].values, dtype=torch.float32)\n",
    "        else:\n",
    "            raise ValueError(\"No data found\")\n",
    "\n",
    "        # convert image to tensor. if transform needed, using transform\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = ToTensor()(image)\n",
    "\n",
    "        return image_id_photo, image, image_features \n",
    "    \n",
    "    # (X - mean) / sd normalization\n",
    "    def normalize(self, df):\n",
    "        normalized_df = df.copy()\n",
    "        # first column is photo id, skip\n",
    "        for column in df.columns[1:]: \n",
    "            mean_value = df[column].mean()\n",
    "            std_value = df[column].std()\n",
    "            normalized_df[column] = (df[column]-mean_value)/std_value\n",
    "        return normalized_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Create data set from data frame\n",
    "amfd_dataset = AMFDDataset(AMFD_Dataframe, image_paths)\n",
    "\n",
    "# using photo ids as unit because we must put photos of one person in one dataset\n",
    "photo_ids = AMFD_Dataframe['PhotoID'].values\n",
    "unique_photo_ids = np.unique(photo_ids)\n",
    "\n",
    "# extract a single set (1/10) for test set\n",
    "train_val_photo_ids, test_photo_ids = train_test_split(unique_photo_ids, test_size=0.1, random_state=42)\n",
    "\n",
    "# extract a single set for validation set.\n",
    "train_photo_ids, val_photo_ids = train_test_split(train_val_photo_ids, test_size=(1/9), random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_indices is [0, 2, 3, 5, 6, 7, 8, 9, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 37, 38, 39, 40, 41, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 81, 82, 83, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 114, 115, 116, 117, 118, 121, 122, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 137, 138, 140, 141, 142, 143, 146, 147, 148, 149, 150, 151, 152, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 172, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 190, 191, 192, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217] and len is 174\n",
      "val_indices is [1, 14, 21, 27, 35, 36, 44, 46, 61, 80, 88, 110, 123, 130, 136, 144, 145, 153, 155, 170, 189, 197] and len is 22\n",
      "test_indices is [4, 10, 11, 30, 45, 64, 68, 78, 79, 84, 96, 113, 119, 120, 139, 154, 173, 177, 187, 188, 193, 205] and len is 22\n",
      "train_dataset is <torch.utils.data.dataset.Subset object at 0x0000023D334E21D0> and len is 174\n",
      "val_dataset is <torch.utils.data.dataset.Subset object at 0x0000023D33EAA010> and len is 22\n",
      "test_dataset is <torch.utils.data.dataset.Subset object at 0x0000023D334E21D0> and len is 22\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# get index from train_photo_ids, val_photo_ids, and test_photo_ids\n",
    "def get_photo_ids(photo_id_sets):\n",
    "    result = []\n",
    "    for i, photo_id in enumerate(photo_ids):\n",
    "        if photo_id in photo_id_sets:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "train_indices = get_photo_ids(train_photo_ids)\n",
    "val_indices = get_photo_ids(val_photo_ids)\n",
    "test_indices = get_photo_ids(test_photo_ids)\n",
    "\n",
    "print(f\"train_indices is {train_indices} and len is {len(train_indices)}\")\n",
    "print(f\"val_indices is {val_indices} and len is {len(val_indices)}\")\n",
    "print(f\"test_indices is {test_indices} and len is {len(test_indices)}\")\n",
    "\n",
    "# using these indices to get element from dataset in order to make photos of one person in one set\n",
    "train_dataset = Subset(amfd_dataset, train_indices)\n",
    "val_dataset = Subset(amfd_dataset, val_indices)\n",
    "test_dataset = Subset(amfd_dataset, test_indices)\n",
    "\n",
    "print(f\"train_dataset is {train_dataset} and len is {len(train_dataset)}\")\n",
    "print(f\"val_dataset is {val_dataset} and len is {len(val_dataset)}\")\n",
    "print(f\"test_dataset is {train_dataset} and len is {len(test_dataset)}\")\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
