{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsn-8QFu26en",
        "outputId": "dcbda452-b28a-40dd-f599-3d5c6558b2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TWXjdqb1iKR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Read excel table and set data to data frame\n",
        "data_dir = '/content/drive/MyDrive/_data/AMFD Faces Final/'\n",
        "AMFD_Dataframe = pd.read_excel( data_dir + 'AMFD Norming Data + Codebook.xlsx', skiprows=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Jo8__o1iKS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# put all image path to a list\n",
        "# image_dir = \"AMFD_Faces_Final\"\n",
        "image_paths = []\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.endswith('.jpg'):\n",
        "        image_paths.append(os.path.join(data_dir, filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbAJXseD1iKS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class AMFDDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_paths, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_path = self.image_paths[idx]\n",
        "\n",
        "        # extract element from the image name\n",
        "        parts = image_path.split('\\\\')[-1].split('-')\n",
        "\n",
        "        # get expression code (which is the Ftype in table)\n",
        "        expression_code = 0 if parts[0][-2] == 'N' else 1\n",
        "        # print(parts)\n",
        "\n",
        "        # get image id from image file\n",
        "        image_id_photo = parts[1].split('.')[0]\n",
        "\n",
        "        # using expression code and image id to seach in dataframe\n",
        "        match = self.dataframe[(self.dataframe['PhotoID'] == int(image_id_photo)) & (self.dataframe['FType'] == expression_code)]\n",
        "\n",
        "        # get the feature from the matching line\n",
        "        image_features = None\n",
        "        if not match.empty:\n",
        "            # this will return the whole line\n",
        "            # if we need photo_id included, use match.iloc[0, 0:].values, otherwise match.iloc[0, 1:].values\n",
        "            image_features = torch.tensor(match.iloc[0, 0:].values, dtype=torch.float32)\n",
        "            # facial_expression = torch.tensor(match.iloc[0, 1], dtype=torch.float32)\n",
        "            attractiveness = torch.tensor(match.iloc[0, 14], dtype=torch.float32)\n",
        "        else:\n",
        "            raise ValueError(\"No data found\")\n",
        "\n",
        "        # convert image to tensor. if transform needed, using transform\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        # convert image to gray scale\n",
        "        # image = image.convert('L')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            image = ToTensor()(image)\n",
        "\n",
        "        return image_id_photo, image, image_features, torch.tensor(expression_code), attractiveness\n",
        "\n",
        "    # (X - mean) / sd normalization\n",
        "    def normalize(self, df):\n",
        "        normalized_df = df.copy()\n",
        "        # first column is photo id, skip\n",
        "        for column in df.columns[1:]:\n",
        "            mean_value = df[column].mean()\n",
        "            std_value = df[column].std()\n",
        "            normalized_df[column] = (df[column]-mean_value)/std_value\n",
        "        return normalized_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPJAh6-J1iKS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Create data set from data frame\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
        "    transforms.ToTensor(),          # Convert the image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])\n",
        "amfd_dataset = AMFDDataset(AMFD_Dataframe, image_paths, transform=transform)\n",
        "\n",
        "# using photo ids as unit because we must put photos of one person in one dataset\n",
        "photo_ids = AMFD_Dataframe['PhotoID'].values\n",
        "unique_photo_ids = np.unique(photo_ids)\n",
        "\n",
        "# extract a single set (1/10) for test set\n",
        "train_val_photo_ids, test_photo_ids = train_test_split(unique_photo_ids, test_size=0.1, random_state=42)\n",
        "\n",
        "# extract a single set for validation set.\n",
        "train_photo_ids, val_photo_ids = train_test_split(train_val_photo_ids, test_size=(1/9), random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrnlbHrP1iKT",
        "outputId": "1d455fac-6898-45f9-bf91-7abd4e7c2d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_indices is [0, 1, 3, 5, 6, 7, 8, 9, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 37, 38, 39, 40, 41, 42, 43, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 80, 81, 82, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 114, 115, 116, 117, 118, 121, 122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 140, 141, 142, 143, 146, 147, 148, 149, 150, 151, 152, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 172, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 190, 191, 192, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218] and len is 175\n",
            "val_indices is [2, 14, 27, 35, 36, 44, 46, 51, 61, 79, 87, 111, 123, 136, 144, 145, 153, 155, 160, 170, 189, 197] and len is 22\n",
            "test_indices is [4, 10, 11, 30, 45, 64, 68, 77, 78, 83, 95, 113, 119, 120, 139, 154, 173, 177, 187, 188, 193, 205] and len is 22\n",
            "train_dataset is <torch.utils.data.dataset.Subset object at 0x7876d35c5510> and len is 175\n",
            "val_dataset is <torch.utils.data.dataset.Subset object at 0x7875df39beb0> and len is 22\n",
            "test_dataset is <torch.utils.data.dataset.Subset object at 0x7876d35c5510> and len is 22\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# get index from train_photo_ids, val_photo_ids, and test_photo_ids\n",
        "def get_photo_ids(photo_id_sets):\n",
        "    result = []\n",
        "    for i, photo_id in enumerate(photo_ids):\n",
        "        if photo_id in photo_id_sets:\n",
        "            result.append(i)\n",
        "    return result\n",
        "\n",
        "train_indices = get_photo_ids(train_photo_ids)\n",
        "val_indices = get_photo_ids(val_photo_ids)\n",
        "test_indices = get_photo_ids(test_photo_ids)\n",
        "\n",
        "print(f\"train_indices is {train_indices} and len is {len(train_indices)}\")\n",
        "print(f\"val_indices is {val_indices} and len is {len(val_indices)}\")\n",
        "print(f\"test_indices is {test_indices} and len is {len(test_indices)}\")\n",
        "\n",
        "# using these indices to get element from dataset in order to make photos of one person in one set\n",
        "train_dataset = Subset(amfd_dataset, train_indices)\n",
        "val_dataset = Subset(amfd_dataset, val_indices)\n",
        "test_dataset = Subset(amfd_dataset, test_indices)\n",
        "\n",
        "print(f\"train_dataset is {train_dataset} and len is {len(train_dataset)}\")\n",
        "print(f\"val_dataset is {val_dataset} and len is {len(val_dataset)}\")\n",
        "print(f\"test_dataset is {train_dataset} and len is {len(test_dataset)}\")\n",
        "\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MyUA0q0-1iKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52541d01-03f4-4a2e-8269-849fe67d1fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0\n",
            "('1093', '1003', '1055', '1001', '1039', '1080', '1076', '1084')\n",
            "torch.Size([8, 3, 224, 224])\n",
            "torch.Size([8, 71])\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 1])\n",
            "tensor([3.8500, 5.4259, 3.6829, 3.8654, 4.4510, 4.8302, 5.7000, 5.0806])\n"
          ]
        }
      ],
      "source": [
        "#@title collapse\n",
        "for i, (image_id_photo, image, image_features, face_expression, attract_score) in enumerate(train_loader):\n",
        "    print(f\"batch {i}\")\n",
        "    print(image_id_photo)\n",
        "    print(image.shape)\n",
        "    print(image_features.shape)\n",
        "    print(face_expression)\n",
        "    print(attract_score)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "vgg = models.vgg16(pretrained=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiW1aQtP4Y1a",
        "outputId": "da186578-a79e-4aa3-9637-63cae8caf846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 190MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomHead(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(CustomHead, self).__init__()\n",
        "        self.common_layer = nn.Sequential(\n",
        "            nn.Linear(num_features, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "        # Output layer for binary classification\n",
        "        self.classifier1 = nn.Linear(4096, 1000)\n",
        "        self.classifier2 = nn.Linear(1000, 2)\n",
        "        # Output layer for regression\n",
        "        self.regressor = nn.Linear(4096, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.common_layer(x)\n",
        "        classification1 = self.classifier1(x)\n",
        "        classification_output = self.classifier2(torch.relu(classification1))\n",
        "        regression_output = self.regressor(x)\n",
        "        return classification_output, regression_output"
      ],
      "metadata": {
        "id": "JxBbDbok7Fvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# facial_expressions = [a[3].item() for i, a in enumerate(train_dataset)]"
      ],
      "metadata": {
        "id": "El_dsZFT8A5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# resnet = models.resnet50(pretrained=True)\n",
        "# num_features = resnet.fc.in_features\n",
        "num_features = vgg.classifier[0].in_features\n",
        "vgg.classifier = CustomHead(num_features)\n",
        "# resnet.fc = nn.Identity()\n",
        "# resnet.fc = CustomHead(num_features)\n",
        "vgg.to(device)\n",
        "# resnet.eval()\n",
        "\n",
        "optimizer = torch.optim.Adam(vgg.parameters(), lr=1e-4,  weight_decay=1e-5)\n",
        "# weight_tensor = torch.tensor(len(facial_expressions)/(np.bincount(facial_expressions)*2), dtype=torch.float32, device=device)\n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "criterion2 = nn.MSELoss()\n",
        "criterion1.to(device)\n",
        "criterion2.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui6JOjqL48ij",
        "outputId": "ce9aaa4d-90d9-4a29-ba07-972a356b36ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MSELoss()"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg2 = models.vgg16(pretrained=True)\n",
        "vgg.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-lWh2lba5hl",
        "outputId": "c175d3e3-371d-4982-fbf7-dd39b53ab236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomHead(\n",
              "  (common_layer): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (classifier1): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  (classifier2): Linear(in_features=1000, out_features=2, bias=True)\n",
              "  (regressor): Linear(in_features=4096, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Train function\n",
        "def train(model, train_loader, val_loader, optimizer, criterion1, criterion2, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for params in model.parameters():\n",
        "            params.requires_grad = False\n",
        "        for params in model.fc.parameters():\n",
        "            params.requires_grad = True\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0\n",
        "        for i, (image_id_photo, image, image_features, face_type, att_score) in enumerate(train_loader):\n",
        "            image = image.to(device)\n",
        "            face_type = face_type.to(device)\n",
        "            att_score = att_score.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out_face, out_score = model(image)\n",
        "            loss = criterion1(out_face.squeeze(), face_type.float()) + criterion2(out_score.squeeze(), att_score)\n",
        "            pred_face = torch.argmax(out_face, dim=1)\n",
        "            train_corrects += torch.sum(pred_face == face_type)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_acc = train_corrects / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        with torch.no_grad():\n",
        "            for i, (image_id_photo, image, image_features, face_type, att_score) in enumerate(val_loader):\n",
        "                image = image.to(device)\n",
        "                face_type = face_type.to(device)\n",
        "                att_score = att_score.to(device)\n",
        "                out_face, out_score = model(image)\n",
        "                loss = criterion1(out_face.squeeze(), face_type.float()) + criterion2(out_score.squeeze(), att_score)\n",
        "                pred_face = torch.argmax(out_face, dim=1)\n",
        "                val_corrects += torch.sum(pred_face == face_type)\n",
        "                val_loss += loss.item()\n",
        "            val_loss = val_loss / len(val_loader)\n",
        "            val_acc = val_corrects / len(val_loader.dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        # print(f\"Epoch {epoch}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hWrglX5l5ia9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = vgg\n",
        "epochs=30\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  for params in model.parameters():\n",
        "      params.requires_grad = False\n",
        "  for params in model.classifier.parameters():\n",
        "      params.requires_grad = True\n",
        "  train_loss = 0.0\n",
        "  train_corrects = 0\n",
        "  for i, (image_id_photo, image, image_features, face_type, att_score) in enumerate(train_loader):\n",
        "      image = image.to(device)\n",
        "      face_type = face_type.to(device)\n",
        "      att_score = att_score.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      out_face, out_score = model(image)\n",
        "      loss = 2 * criterion1(out_face.squeeze(), face_type) + criterion2(out_score.squeeze(), att_score)\n",
        "      pred_face = torch.argmax(out_face, dim=1)\n",
        "      train_corrects += torch.sum(pred_face == face_type)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "  train_loss = train_loss / len(train_loader)\n",
        "  train_acc = train_corrects / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  val_corrects = 0\n",
        "  with torch.no_grad():\n",
        "      for i, (image_id_photo, image, image_features, face_type, att_score) in enumerate(val_loader):\n",
        "          image = image.to(device)\n",
        "          face_type = face_type.to(device)\n",
        "          att_score = att_score.to(device)\n",
        "          out_face, out_score = model(image)\n",
        "          loss = criterion1(out_face.squeeze(), face_type) + criterion2(out_score.squeeze(), att_score)\n",
        "          pred_face = torch.argmax(out_face, dim=1)\n",
        "          val_corrects += torch.sum(pred_face == face_type)\n",
        "          val_loss += loss.item()\n",
        "      val_loss = val_loss / len(val_loader)\n",
        "      val_acc = val_corrects / len(val_loader.dataset)\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9oABKI5On6a",
        "outputId": "7e41ca03-1bad-4f31-de91-1cba7e356b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Train Loss: 1.3849, Train Acc: 0.6800 Val Loss: 1.1347, Val Acc: 0.5455\n",
            "Epoch 2/30, Train Loss: 1.0967, Train Acc: 0.8686 Val Loss: 1.1214, Val Acc: 0.5455\n",
            "Epoch 3/30, Train Loss: 0.9158, Train Acc: 0.8514 Val Loss: 1.4750, Val Acc: 0.5455\n",
            "Epoch 4/30, Train Loss: 0.4906, Train Acc: 0.9429 Val Loss: 1.5060, Val Acc: 0.5909\n",
            "Epoch 5/30, Train Loss: 0.5475, Train Acc: 0.9200 Val Loss: 2.5583, Val Acc: 0.5909\n",
            "Epoch 6/30, Train Loss: 1.7173, Train Acc: 0.7543 Val Loss: 1.3248, Val Acc: 0.6364\n",
            "Epoch 7/30, Train Loss: 0.9209, Train Acc: 0.8800 Val Loss: 1.1708, Val Acc: 0.5909\n",
            "Epoch 8/30, Train Loss: 0.5728, Train Acc: 0.9429 Val Loss: 1.4763, Val Acc: 0.6364\n",
            "Epoch 9/30, Train Loss: 0.4100, Train Acc: 0.9657 Val Loss: 1.5994, Val Acc: 0.5909\n",
            "Epoch 10/30, Train Loss: 0.4664, Train Acc: 0.9486 Val Loss: 1.2952, Val Acc: 0.6818\n",
            "Epoch 11/30, Train Loss: 0.3272, Train Acc: 0.9771 Val Loss: 1.4529, Val Acc: 0.6364\n",
            "Epoch 12/30, Train Loss: 0.2857, Train Acc: 0.9886 Val Loss: 1.4433, Val Acc: 0.6818\n",
            "Epoch 13/30, Train Loss: 0.2941, Train Acc: 1.0000 Val Loss: 1.3776, Val Acc: 0.7273\n",
            "Epoch 14/30, Train Loss: 0.2665, Train Acc: 1.0000 Val Loss: 1.6351, Val Acc: 0.7273\n",
            "Epoch 15/30, Train Loss: 0.2406, Train Acc: 1.0000 Val Loss: 1.6660, Val Acc: 0.6818\n",
            "Epoch 16/30, Train Loss: 0.2373, Train Acc: 1.0000 Val Loss: 1.5960, Val Acc: 0.7273\n",
            "Epoch 17/30, Train Loss: 0.2176, Train Acc: 0.9886 Val Loss: 1.9230, Val Acc: 0.6364\n",
            "Epoch 18/30, Train Loss: 0.3836, Train Acc: 0.9886 Val Loss: 1.8930, Val Acc: 0.6364\n",
            "Epoch 19/30, Train Loss: 0.3517, Train Acc: 0.9829 Val Loss: 2.1940, Val Acc: 0.7273\n",
            "Epoch 20/30, Train Loss: 0.2521, Train Acc: 0.9943 Val Loss: 2.9798, Val Acc: 0.5455\n",
            "Epoch 21/30, Train Loss: 0.3336, Train Acc: 0.9943 Val Loss: 1.9696, Val Acc: 0.6364\n",
            "Epoch 22/30, Train Loss: 0.2822, Train Acc: 0.9829 Val Loss: 2.4646, Val Acc: 0.6364\n",
            "Epoch 23/30, Train Loss: 0.3123, Train Acc: 0.9886 Val Loss: 2.4488, Val Acc: 0.7273\n",
            "Epoch 24/30, Train Loss: 0.2307, Train Acc: 0.9943 Val Loss: 2.6862, Val Acc: 0.5909\n",
            "Epoch 25/30, Train Loss: 0.4333, Train Acc: 0.9771 Val Loss: 2.1066, Val Acc: 0.6364\n",
            "Epoch 26/30, Train Loss: 0.2665, Train Acc: 0.9829 Val Loss: 2.0774, Val Acc: 0.6818\n",
            "Epoch 27/30, Train Loss: 0.2305, Train Acc: 1.0000 Val Loss: 2.4718, Val Acc: 0.5909\n",
            "Epoch 28/30, Train Loss: 0.2299, Train Acc: 1.0000 Val Loss: 2.1166, Val Acc: 0.6364\n",
            "Epoch 29/30, Train Loss: 0.2096, Train Acc: 1.0000 Val Loss: 2.4780, Val Acc: 0.6818\n",
            "Epoch 30/30, Train Loss: 0.2040, Train Acc: 1.0000 Val Loss: 2.5296, Val Acc: 0.5909\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}